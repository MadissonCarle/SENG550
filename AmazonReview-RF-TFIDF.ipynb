{"cells":[{"cell_type":"code","source":["!pip install nltk # install nltk onto databricks\nimport nltk\nnltk.download('stopwords') #add stopwords\n\nimport os.path\nimport sklearn\nfrom pyspark.sql.types import *\nfrom nltk.stem import LancasterStemmer\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport string\nfrom pyspark.sql.functions import concat_ws, udf\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5f998ce3-634e-4537-89fe-02f917e76d10","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Requirement already satisfied: nltk in /databricks/python3/lib/python3.8/site-packages (3.8)\r\nRequirement already satisfied: joblib in /databricks/python3/lib/python3.8/site-packages (from nltk) (1.0.1)\r\nRequirement already satisfied: click in /databricks/python3/lib/python3.8/site-packages (from nltk) (8.1.3)\r\nRequirement already satisfied: regex>=2021.8.3 in /databricks/python3/lib/python3.8/site-packages (from nltk) (2022.10.31)\r\nRequirement already satisfied: tqdm in /databricks/python3/lib/python3.8/site-packages (from nltk) (4.64.1)\r\n\u001B[33mWARNING: You are using pip version 21.0.1; however, version 22.3.1 is available.\r\nYou should consider upgrading via the '/databricks/python3/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: nltk in /databricks/python3/lib/python3.8/site-packages (3.8)\r\nRequirement already satisfied: joblib in /databricks/python3/lib/python3.8/site-packages (from nltk) (1.0.1)\r\nRequirement already satisfied: click in /databricks/python3/lib/python3.8/site-packages (from nltk) (8.1.3)\r\nRequirement already satisfied: regex>=2021.8.3 in /databricks/python3/lib/python3.8/site-packages (from nltk) (2022.10.31)\r\nRequirement already satisfied: tqdm in /databricks/python3/lib/python3.8/site-packages (from nltk) (4.64.1)\r\n\u001B[33mWARNING: You are using pip version 21.0.1; however, version 22.3.1 is available.\r\nYou should consider upgrading via the '/databricks/python3/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"]}}],"execution_count":0},{"cell_type":"code","source":["#list of words to filter out of statements\nstop_words = set(stopwords.words('english'))\n# stemmer to turn words into their roots\nlan=LancasterStemmer()\n\n@udf\ndef clean(text):\n    print(text)\n    text = text.lower()#convert to lowercase\n    text = re.sub(r\"http\\S+|www\\S+|https\\S+|ftp\\S+\", '', text, flags=re.MULTILINE)#remove links\n    text = re.sub('[^\\w ]','',text)#remove whitespaces \n    text = text.translate(str.maketrans('', '', string.punctuation))#remove punctuation \n    tweet_tokens = word_tokenize(text,preserve_line=True)#tokenize words\n    filtered_words = [lan.stem(w) for w in tweet_tokens if not w in stop_words]#stemming\n\n    return ' '.join(filtered_words)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3ba445b9-5220-4a8c-adf1-87f2ca67e1e3","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["reviewsDF = spark.read.csv('/FileStore/tables/reviews-2.csv', header = True) #read in dataset\nreviewsDF = reviewsDF.select('overall',concat_ws(' ',reviewsDF['summary'],reviewsDF['reviewText']).alias(\"text\"))#combine summary and reviewText\nreviewsDF = reviewsDF.withColumn('text', clean(reviewsDF['text'])) #clean text data\nreviewsDF.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cbc4f217-e1f5-48e8-8ce3-a250c110e5de","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------+--------------------+\n|overall|                text|\n+-------+--------------------+\n|      5|fiv star advert r...|\n|      5|good fac lik od f...|\n|      1|smel aw bought sm...|\n|      5|tru noth lik aqu ...|\n|      5|bvlgari shampoo e...|\n+-------+--------------------+\nonly showing top 5 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------+--------------------+\n|overall|                text|\n+-------+--------------------+\n|      5|fiv star advert r...|\n|      5|good fac lik od f...|\n|      1|smel aw bought sm...|\n|      5|tru noth lik aqu ...|\n|      5|bvlgari shampoo e...|\n+-------+--------------------+\nonly showing top 5 rows\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n#tokenize words\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\nwordsData = tokenizer.transform(reviewsDF)\n#convert to vector\nhashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\nfeaturizedData = hashingTF.transform(wordsData)\n#copute IDF of rawFeatures\nidf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\nidfModel = idf.fit(featurizedData)\nrescaledData = idfModel.transform(featurizedData)\n#add to RDD\nreviewsRDD = rescaledData.select(\"overall\", \"features\").rdd\nreviewsRDD.take(1)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a0888897-3d0b-4857-893e-b6e827f4af47","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[4]: [Row(overall='5', features=SparseVector(20, {0: 0.5772, 4: 0.7366, 10: 0.3344, 15: 0.4318, 19: 0.6349}))]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[4]: [Row(overall='5', features=SparseVector(20, {0: 0.5772, 4: 0.7366, 10: 0.3344, 15: 0.4318, 19: 0.6349}))]"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.mllib.linalg import Vectors\nfrom pyspark.mllib.regression import LabeledPoint\n\n\ntransformedRDD = reviewsRDD.map(lambda row: LabeledPoint(row['overall'], row['features'].toArray()))\n#split data (60% train 20% validation 20% test)\nsplits = [0.6, 0.2, 0.2]\ntraining_data, validation_data, test_data = transformedRDD.randomSplit(splits, 0)\n\n#cache to keep in mem\ntraining_data.cache()\nvalidation_data.cache()\ntest_data.cache()\n\nprint(\"Number of training set rows: %d\" % training_data.count())\nprint(\"Number of validation set rows: %d\" % validation_data.count())\nprint(\"Number of test set rows: %d\" % test_data.count())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"33ee86ec-37a7-444f-842b-8ffca7cf459a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Number of training set rows: 3119\nNumber of validation set rows: 1077\nNumber of test set rows: 1073\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Number of training set rows: 3119\nNumber of validation set rows: 1077\nNumber of test set rows: 1073\n"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.mllib.evaluation import MulticlassMetrics\n\ndef evaluateMetrics(metrics, m_id):\n    print()\n    print(\"--------------------------------------------------------------\")    \n    print(\"Performance Metrics for Model \" + str(m_id))\n    print()\n    print(\"Accuracy: %.2f\" % (metrics.accuracy))\n    for i in range(1, 6):\n        print(\"Label \" + str(i))\n        print(\"Precision: %.2f\" % (metrics.precision(i)))\n        print(\"Recall: %.2f\" % (metrics.recall(i)))\n       # print(\"F-Score: %.2f%%\" % (metrics.fMeasure(i,beta=1.0) * 100))\n        print()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8f709c32-0a51-43d8-b345-acbe8bb5cbca","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.mllib.tree import RandomForest\n#evaluate which parameters work best for model \nmodel1 = RandomForest.trainClassifier(sc.parallelize(training_data.collect()), numClasses=6, categoricalFeaturesInfo={}, numTrees=10, maxDepth=5)\nmodel2 = RandomForest.trainClassifier(sc.parallelize(training_data.collect()), numClasses=6, categoricalFeaturesInfo={}, numTrees=10, maxDepth=20)\nmodel3 = RandomForest.trainClassifier(sc.parallelize(training_data.collect()), numClasses=6, categoricalFeaturesInfo={}, numTrees=20, maxDepth=25)\nmodel4 = RandomForest.trainClassifier(sc.parallelize(training_data.collect()), numClasses=6, categoricalFeaturesInfo={}, numTrees=25, maxDepth=30)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c7dde3ae-66e9-4a6b-9512-064ea958ff64","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Model accuracy: 89.47%\nModel accuracy: 95.90%\nModel accuracy: 95.99%\nModel accuracy: 95.99%\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Model accuracy: 89.47%\nModel accuracy: 95.90%\nModel accuracy: 95.99%\nModel accuracy: 95.99%\n"]}}],"execution_count":0},{"cell_type":"code","source":["#evaluate models using validation data\npred = model1.predict(validation_data.map(lambda x: x.features))\ny_labels = validation_data.map(lambda x: x.label)\npredAndLab = pred.zip(y_labels)\nvalMetrics = MulticlassMetrics(predAndLab)\nevaluateMetrics(valMetrics, 1)\n\npred = model2.predict(validation_data.map(lambda x: x.features))\ny_labels = validation_data.map(lambda x: x.label)\npredAndLab = pred.zip(y_labels)\nvalMetrics = MulticlassMetrics(predAndLab)\nevaluateMetrics(valMetrics, 2)\n\npred = model3.predict(validation_data.map(lambda x: x.features))\ny_labels = validation_data.map(lambda x: x.label)\npredAndLab = pred.zip(y_labels)\nvalMetrics = MulticlassMetrics(predAndLab)\nevaluateMetrics(valMetrics, 3)\n\npred = model4.predict(validation_data.map(lambda x: x.features))\ny_labels = validation_data.map(lambda x: x.label)\npredAndLab = pred.zip(y_labels)\nvalMetrics = MulticlassMetrics(predAndLab)\nevaluateMetrics(valMetrics, 4)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"daa93fad-b16f-404b-bdd9-05a25496d7c2","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"\n--------------------------------------------------------------\nPerformance Metrics for Model 1\n\nAccuracy: 0.89\nLabel 1\nPrecision: 0.00\nRecall: 0.00\n\nLabel 2\nPrecision: 0.00\nRecall: 0.00\n\nLabel 3\nPrecision: 1.00\nRecall: 0.04\n\nLabel 4\nPrecision: 1.00\nRecall: 0.03\n\nLabel 5\nPrecision: 0.89\nRecall: 1.00\n\n\n--------------------------------------------------------------\nPerformance Metrics for Model 2\n\nAccuracy: 0.96\nLabel 1\nPrecision: 1.00\nRecall: 0.74\n\nLabel 2\nPrecision: 1.00\nRecall: 0.40\n\nLabel 3\nPrecision: 0.93\nRecall: 0.56\n\nLabel 4\nPrecision: 0.91\nRecall: 0.64\n\nLabel 5\nPrecision: 0.96\nRecall: 1.00\n\n\n--------------------------------------------------------------\nPerformance Metrics for Model 3\n\nAccuracy: 0.96\nLabel 1\nPrecision: 1.00\nRecall: 0.74\n\nLabel 2\nPrecision: 1.00\nRecall: 0.50\n\nLabel 3\nPrecision: 0.94\nRecall: 0.60\n\nLabel 4\nPrecision: 0.95\nRecall: 0.62\n\nLabel 5\nPrecision: 0.96\nRecall: 1.00\n\n\n--------------------------------------------------------------\nPerformance Metrics for Model 4\n\nAccuracy: 0.96\nLabel 1\nPrecision: 1.00\nRecall: 0.74\n\nLabel 2\nPrecision: 1.00\nRecall: 0.50\n\nLabel 3\nPrecision: 1.00\nRecall: 0.60\n\nLabel 4\nPrecision: 0.98\nRecall: 0.61\n\nLabel 5\nPrecision: 0.95\nRecall: 1.00\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\n--------------------------------------------------------------\nPerformance Metrics for Model 1\n\nAccuracy: 0.89\nLabel 1\nPrecision: 0.00\nRecall: 0.00\n\nLabel 2\nPrecision: 0.00\nRecall: 0.00\n\nLabel 3\nPrecision: 1.00\nRecall: 0.04\n\nLabel 4\nPrecision: 1.00\nRecall: 0.03\n\nLabel 5\nPrecision: 0.89\nRecall: 1.00\n\n\n--------------------------------------------------------------\nPerformance Metrics for Model 2\n\nAccuracy: 0.96\nLabel 1\nPrecision: 1.00\nRecall: 0.74\n\nLabel 2\nPrecision: 1.00\nRecall: 0.40\n\nLabel 3\nPrecision: 0.93\nRecall: 0.56\n\nLabel 4\nPrecision: 0.91\nRecall: 0.64\n\nLabel 5\nPrecision: 0.96\nRecall: 1.00\n\n\n--------------------------------------------------------------\nPerformance Metrics for Model 3\n\nAccuracy: 0.96\nLabel 1\nPrecision: 1.00\nRecall: 0.74\n\nLabel 2\nPrecision: 1.00\nRecall: 0.50\n\nLabel 3\nPrecision: 0.94\nRecall: 0.60\n\nLabel 4\nPrecision: 0.95\nRecall: 0.62\n\nLabel 5\nPrecision: 0.96\nRecall: 1.00\n\n\n--------------------------------------------------------------\nPerformance Metrics for Model 4\n\nAccuracy: 0.96\nLabel 1\nPrecision: 1.00\nRecall: 0.74\n\nLabel 2\nPrecision: 1.00\nRecall: 0.50\n\nLabel 3\nPrecision: 1.00\nRecall: 0.60\n\nLabel 4\nPrecision: 0.98\nRecall: 0.61\n\nLabel 5\nPrecision: 0.95\nRecall: 1.00\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.mllib.tree import RandomForest\n#Train RandomForest Model\n#using parameters with best results from above\n#classes represent star ratings\n#set number of trees for eval to 10 and set maxDepth to 20\n#set random seed to 0 so results can be replicated\nmodel = RandomForest.trainClassifier(sc.parallelize(training_data.collect()), numClasses=6, categoricalFeaturesInfo={}, numTrees=25, maxDepth=30, seed=0)\n\n#evaluate models using test data\npred = model.predict(test_data.map(lambda x: x.features))\ny_labels = test_data.map(lambda x: x.label)\npredAndLab = pred.zip(y_labels)\nvalMetrics = MulticlassMetrics(predAndLab)\nevaluateMetrics(valMetrics, 5)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"aa9340ce-5559-48b4-b2cd-bbd9834c3af0","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"\n--------------------------------------------------------------\nPerformance Metrics for Model 5\n\nAccuracy: 0.97\nLabel 1\nPrecision: 1.00\nRecall: 0.88\n\nLabel 2\nPrecision: 1.00\nRecall: 0.94\n\nLabel 3\nPrecision: 1.00\nRecall: 0.63\n\nLabel 4\nPrecision: 0.93\nRecall: 0.69\n\nLabel 5\nPrecision: 0.97\nRecall: 1.00\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\n--------------------------------------------------------------\nPerformance Metrics for Model 5\n\nAccuracy: 0.97\nLabel 1\nPrecision: 1.00\nRecall: 0.88\n\nLabel 2\nPrecision: 1.00\nRecall: 0.94\n\nLabel 3\nPrecision: 1.00\nRecall: 0.63\n\nLabel 4\nPrecision: 0.93\nRecall: 0.69\n\nLabel 5\nPrecision: 0.97\nRecall: 1.00\n\n"]}}],"execution_count":0}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.10.8","nbconvert_exporter":"python","file_extension":".py"},"vscode":{"interpreter":{"hash":"e7af9e86c3681899aba80f8b0067881b136bce65c27e78bb175258adc8d4bb5d"}},"application/vnd.databricks.v1+notebook":{"notebookName":"AmazonReview-RF-TFIDF (1) (2)","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":963298282021446}},"nbformat":4,"nbformat_minor":0}
