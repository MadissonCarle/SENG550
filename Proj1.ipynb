{"cells":[{"cell_type":"code","source":["# /FileStore/tables/beauty4.csv\nimport os.path\n# baseDir = os.path.join('data')\n# inputPath = os.path.join('SENG501', 'lab1', 'deerfoot.csv')\nfileName = '/FileStore/tables/beauty4.csv'\n\nreviewsRDD = sc.textFile(fileName)\ntrailed = reviewsRDD.map(lambda x: x.rstrip(\"'\"))\nprint(trailed.take(1))\nprint(trailed.count())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"387b152b-83c1-43be-81e6-3a3bf1d56f3d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"[\"'5','True','09 1, 2016','A3CIUOJXQ5VDQ2','B0000530HU','{''Size:'': '' 7.0 oz'', ''Flavor:'': '' Classic Ice Blue''}','Shelly F','As advertised. Reasonably priced','Five Stars','1472688000','',\"]\n5269\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["[\"'5','True','09 1, 2016','A3CIUOJXQ5VDQ2','B0000530HU','{''Size:'': '' 7.0 oz'', ''Flavor:'': '' Classic Ice Blue''}','Shelly F','As advertised. Reasonably priced','Five Stars','1472688000','',\"]\n5269\n"]}}],"execution_count":0},{"cell_type":"code","source":["# TODO: Replace <FILL IN> with appropriate code\ndef extractFields(reviewsRDD):\n    fieldsList = reviewsRDD.split(\"','\")\n    print(len(fieldsList))\n    rate = fieldsList[0].replace(\"'\",\"\")\n    return (rate, fieldsList[7], fieldsList[8])\n     \nprint(extractFields(reviewsRDD.take(1)[0]))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"519feb0e-332c-44ad-a2f9-b11b7eb21f57","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"12\n('5', 'As advertised. Reasonably priced', 'Five Stars')\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["12\n('5', 'As advertised. Reasonably priced', 'Five Stars')\n"]}}],"execution_count":0},{"cell_type":"code","source":["reviewsRatingRDD = reviewsRDD.map(extractFields)\nprint(reviewsRatingRDD.take(3))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a043fed7-dcdb-4d30-963a-1fc6f5cd32f0","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"[('5', 'As advertised. Reasonably priced', 'Five Stars'), ('5', 'Like the oder and the feel when I put it on my face.  I have tried other brands but the reviews from people I know they prefer the oder of this brand. Not hard on the face when dry.  Does not leave dry skin.', 'Good for the face'), ('1', 'I bought this to smell nice after I shave.  When I put it on I smelled awful.  I am 19 and I smelled like a grandmother with too much perfume.', 'Smells awful')]\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["[('5', 'As advertised. Reasonably priced', 'Five Stars'), ('5', 'Like the oder and the feel when I put it on my face.  I have tried other brands but the reviews from people I know they prefer the oder of this brand. Not hard on the face when dry.  Does not leave dry skin.', 'Good for the face'), ('1', 'I bought this to smell nice after I shave.  When I put it on I smelled awful.  I am 19 and I smelled like a grandmother with too much perfume.', 'Smells awful')]\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Text Preprocessing and TFIDF\n###### Make all text from review-text and summary-text lowercase and remove all punctuation\n###### do tfidf\n###### ref: <https://spark.apache.org/docs/latest/mllib-feature-extraction.html>"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f0eff153-70d8-49e5-ba2b-60ece616d84a","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["import string\nfrom pyspark.mllib.feature import HashingTF, IDF\n\n# remove punctuation and make everything lowercase\n\ndef prepData(rdd):\n    r1 = rdd[1].translate(str.maketrans('','',string.punctuation))\n    r2 = rdd[2].translate(str.maketrans('','',string.punctuation))\n    rev = r2.lower() + ' ' + r1.lower()\n    # rev = rev.translate(str.maketrans('','',string.punctuation))\n    label = int(rdd[0])\n    return (label, rev)\n\n\nreviewsData = reviewsRatingRDD.map(prepData)\nprint(reviewsData.take(2))\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"72d8e283-e2a5-4f7a-8499-d7305eb82181","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"[(5, 'five stars as advertised reasonably priced'), (5, 'good for the face like the oder and the feel when i put it on my face  i have tried other brands but the reviews from people i know they prefer the oder of this brand not hard on the face when dry  does not leave dry skin')]\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["[(5, 'five stars as advertised reasonably priced'), (5, 'good for the face like the oder and the feel when i put it on my face  i have tried other brands but the reviews from people i know they prefer the oder of this brand not hard on the face when dry  does not leave dry skin')]\n"]}}],"execution_count":0},{"cell_type":"code","source":["rWordList2 = reviewsData.map(lambda x: x[1].split())\n# print(rWordList2.take(2))\n\ntf = HashingTF()\ntfRDD = tf.transform(rWordList2)\ntfRDD.cache()\nidf = IDF().fit(tfRDD)\ntfidf = idf.transform(tfRDD)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f2a0522f-50b7-4a5c-9b08-09e5d6b53c32","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["print(reviewsData.take(3)[1][1])\nprint(rWordList2.take(3)[1])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0e8b92f1-ff4f-43f3-9112-d76b874cd643","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"like the oder and the feel when i put it on my face  i have tried other brands but the reviews from people i know they prefer the oder of this brand not hard on the face when dry  does not leave dry skin good for the face\n['like', 'the', 'oder', 'and', 'the', 'feel', 'when', 'i', 'put', 'it', 'on', 'my', 'face', 'i', 'have', 'tried', 'other', 'brands', 'but', 'the', 'reviews', 'from', 'people', 'i', 'know', 'they', 'prefer', 'the', 'oder', 'of', 'this', 'brand', 'not', 'hard', 'on', 'the', 'face', 'when', 'dry', 'does', 'not', 'leave', 'dry', 'skin', 'good', 'for', 'the', 'face']\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["like the oder and the feel when i put it on my face  i have tried other brands but the reviews from people i know they prefer the oder of this brand not hard on the face when dry  does not leave dry skin good for the face\n['like', 'the', 'oder', 'and', 'the', 'feel', 'when', 'i', 'put', 'it', 'on', 'my', 'face', 'i', 'have', 'tried', 'other', 'brands', 'but', 'the', 'reviews', 'from', 'people', 'i', 'know', 'they', 'prefer', 'the', 'oder', 'of', 'this', 'brand', 'not', 'hard', 'on', 'the', 'face', 'when', 'dry', 'does', 'not', 'leave', 'dry', 'skin', 'good', 'for', 'the', 'face']\n"]}}],"execution_count":0},{"cell_type":"code","source":["# testing\nr1 = rWordList2.zip(tfidf)\ns = r1.take(3)\nprint(s[0][1])\nprint(s[0][0])\n# print(r1.take(1)[0])\n# print(tfidf.take(2))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0c759d75-eccc-44d6-991c-70c441af64e3","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"(1048576,[13727,246493,665090,680533,761597,929834],[1.2679632993974759,6.778026172307353,5.168588259873252,5.679413883639243,1.1716115485649428,1.8952242497209821])\n['as', 'advertised', 'reasonably', 'priced', 'five', 'stars']\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["(1048576,[13727,246493,665090,680533,761597,929834],[1.2679632993974759,6.778026172307353,5.168588259873252,5.679413883639243,1.1716115485649428,1.8952242497209821])\n['as', 'advertised', 'reasonably', 'priced', 'five', 'stars']\n"]}}],"execution_count":0},{"cell_type":"code","source":["r2 = reviewsData.zip(tfidf)\nm = r2.take(3)\nprint(m[0][0][0])\nprint(m[0][1])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c461c5ab-5836-45a3-b286-589b3c0996df","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"5\n(1048576,[13727,246493,665090,680533,761597,929834],[1.2679632993974759,6.778026172307353,5.168588259873252,5.679413883639243,1.1716115485649428,1.8952242497209821])\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["5\n(1048576,[13727,246493,665090,680533,761597,929834],[1.2679632993974759,6.778026172307353,5.168588259873252,5.679413883639243,1.1716115485649428,1.8952242497209821])\n"]}}],"execution_count":0},{"cell_type":"code","source":["\"\"\"\nrText = reviewsRatingRDD.map(prepText)\nprint(rText.take(2))\n\nrWordList = rText.map(lambda x: x.split(\" \"))\ntf = HashingTF().transform(rWordList)\ntf.cache()\nidf = IDF().fit(tf)\ntfidf = idf.transform(tf)\n\"\"\"\n\n# spark.mllib's IDF implementation provides an option for ignoring terms\n# which occur in less than a minimum number of documents.\n# In such cases, the IDF for these terms is set to 0.\n# This feature can be used by passing the minDocFreq value to the IDF constructor.\n# idfIgnore = IDF(minDocFreq=2).fit(tf)\n# tfidfIgnore = idfIgnore.transform(tf)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d21f4c7d-1f9b-4c3e-b763-446839beea48","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# Doing ML"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"90ae9f61-26d3-45f9-99cc-dd980688af1d","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## Random Forest Model\n##### ref: <https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.mllib.tree.RandomForest.html#pyspark.mllib.tree.RandomForest>"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"34814518-d5c2-4022-8184-34e24e58ec71","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.mllib.regression import LabeledPoint\n\n\ndef parsePoint(line):\n    val = LabeledPoint(line[0][0], line[1])\n    return val\n\nfeaturizedRDD = r2.map(parsePoint)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0782c853-f2d8-4fa0-a6b7-22d7d140dc48","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["print(featurizedRDD.take(1))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b4985245-e3b9-44a6-a77c-d7fe693412ff","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"[LabeledPoint(5.0, (1048576,[13727,246493,665090,680533,761597,929834],[1.2679632993974759,6.778026172307353,5.168588259873252,5.679413883639243,1.1716115485649428,1.8952242497209821]))]\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["[LabeledPoint(5.0, (1048576,[13727,246493,665090,680533,761597,929834],[1.2679632993974759,6.778026172307353,5.168588259873252,5.679413883639243,1.1716115485649428,1.8952242497209821]))]\n"]}}],"execution_count":0},{"cell_type":"code","source":["sample = r2.map(parsePoint)\nprint(sample.take(1)[0].label)\nprint(sample.take(1)[0].features)\nprint(len(sample.take(1)[0].features))\nprint(type(sample.take(1)[0].features))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cc1f20b9-0813-4c83-abbb-5c67d9e68880","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<class 'pyspark.mllib.linalg.SparseVector'>\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["<class 'pyspark.mllib.linalg.SparseVector'>\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Splitting Data"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"15471b4e-c0a9-4670-bb72-7c9f595f80ac","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["weights = [0.6, 0.2, 0.2]\nseed = 42\ntrainData, valData, testData = featurizedRDD.randomSplit(weights, seed)\n\ntrainData.cache()\nvalData.cache()\ntestData.cache()\n\nnTrain = trainData.count()\nnVal = valData.count()\nnTest = testData.count()\n\nprint(nTrain, nVal, nTest, nTrain + nVal + nTest)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5b02ac5a-0729-4118-8bc7-23bbec352ee2","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"3151 1042 1076 5269\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["3151 1042 1076 5269\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Start training model\n##### ref <https://spark.apache.org/docs/latest/mllib-ensembles.html#classification>"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b8475f09-5384-4cc3-bec5-d537125f233f","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.mllib.tree import RandomForest, RandomForestModel\nimport numpy as np\n\nmodel = RandomForest.trainClassifier(trainData, 6, {}, 10)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8ea2de3a-a559-4686-9469-dff856d2527f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-3857930735326542>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[0mlabelsAndPred\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mvalData\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mlp\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mlp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlabel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mzip\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpredictions\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 10\u001B[0;31m \u001B[0mEv\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlabelsandPred\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfilter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mlp\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mlp\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m!=\u001B[0m \u001B[0mlp\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcount\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m/\u001B[0m \u001B[0mfloat\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mvalData\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcount\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     11\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'Test Err:'\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mEv\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'Forest Model:'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mNameError\u001B[0m: name 'labelsandPred' is not defined","errorSummary":"<span class='ansi-red-fg'>NameError</span>: name 'labelsandPred' is not defined","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-3857930735326542>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[0mlabelsAndPred\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mvalData\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mlp\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mlp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlabel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mzip\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpredictions\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 10\u001B[0;31m \u001B[0mEv\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlabelsandPred\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfilter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mlp\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mlp\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m!=\u001B[0m \u001B[0mlp\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcount\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m/\u001B[0m \u001B[0mfloat\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mvalData\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcount\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     11\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'Test Err:'\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mEv\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'Forest Model:'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mNameError\u001B[0m: name 'labelsandPred' is not defined"]}}],"execution_count":0},{"cell_type":"code","source":["model2 = RandomForest.trainClassifier(trainData, numClasses=6, categoricalFeaturesInfo={}, numTrees=20, maxDepth=4)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"25f227d7-fb0e-417f-9750-7aee02ebc537","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["model3 = RandomForest.trainClassifier(trainData, numClasses=6, categoricalFeaturesInfo={}, numTrees=10, maxDepth=7)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"96d57955-be42-4248-b579-c031125fe194","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#evaluation\npredictions = model.predict(valData.map(lambda x: x.features))\nlabelsAndPred = valData.map(lambda lp: lp.label).zip(predictions)\n\npredictions2 = model2.predict(valData.map(lambda x: x.features))\nlabelsAndPred2 = valData.map(lambda lp: lp.label).zip(predictions2)\n\npredictions3 = model3.predict(valData.map(lambda x: x.features))\nlabelsAndPred3 = valData.map(lambda lp: lp.label).zip(predictions3)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6b69d0b9-c956-4798-b7be-2de47d985319","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.mllib.evaluation import MulticlassMetrics\n\nmetrics = MulticlassMetrics(labelsAndPred)\nacc = metrics.accuracy * 100\nerr = 100 - acc\nprint(\"Model 1: \")\nprint(\"Accuracy: %.2f%%\" % acc)\nprint(\"Error: %.2f%%\" % err)\n\nmetrics2 = MulticlassMetrics(labelsAndPred2)\nacc2 = metrics2.accuracy * 100\nerr2 = 100 - acc2\n\nprint(\"Model 2: \")\nprint(\"Accuracy: %.2f%%\" % acc2)\nprint(\"Error: %.2f%%\" % err2)\n\nmetrics3 = MulticlassMetrics(labelsAndPred3)\nacc3 = metrics3.accuracy * 100\nerr3 = 100 - acc3\n\nprint(\"Model 3: \")\nprint(\"Accuracy: %.2f%%\" % acc3)\nprint(\"Error: %.2f%%\" % err3)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"514042e4-32d7-4806-8cda-78782ae67b19","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Model 1: \nAccuracy: 88.29%\nError: 11.71%\nModel 2: \nAccuracy: 88.29%\nError: 11.71%\nModel 3: \nAccuracy: 88.29%\nError: 11.71%\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Model 1: \nAccuracy: 88.29%\nError: 11.71%\nModel 2: \nAccuracy: 88.29%\nError: 11.71%\nModel 3: \nAccuracy: 88.29%\nError: 11.71%\n"]}}],"execution_count":0},{"cell_type":"code","source":["\npredictionAndLabels = valData.map(lambda lp: (float(model.predict(lp.features)), lp.label))\nmetrics2 = MulticlassMetrics(predictionAndLabels)\nprec = metrics2.precision()\nprint(\"precision: %.2f%%\" % prec * 100)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7b7491d8-ac3a-4eec-87a2-059a2283074e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Traceback (most recent call last):\n  File \"/databricks/spark/python/pyspark/serializers.py\", line 473, in dumps\n    return cloudpickle.dumps(obj, pickle_protocol)\n  File \"/databricks/spark/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 73, in dumps\n    cp.dump(obj)\n  File \"/databricks/spark/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 563, in dump\n    return Pickler.dump(self, obj)\n  File \"/databricks/spark/python/pyspark/context.py\", line 369, in __getnewargs__\n    raise RuntimeError(\nRuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Traceback (most recent call last):\n  File \"/databricks/spark/python/pyspark/serializers.py\", line 473, in dumps\n    return cloudpickle.dumps(obj, pickle_protocol)\n  File \"/databricks/spark/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 73, in dumps\n    cp.dump(obj)\n  File \"/databricks/spark/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 563, in dump\n    return Pickler.dump(self, obj)\n  File \"/databricks/spark/python/pyspark/context.py\", line 369, in __getnewargs__\n    raise RuntimeError(\nRuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.\n"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)\n\u001B[0;32m/databricks/spark/python/pyspark/serializers.py\u001B[0m in \u001B[0;36mdumps\u001B[0;34m(self, obj)\u001B[0m\n\u001B[1;32m    472\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 473\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mcloudpickle\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdumps\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobj\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpickle_protocol\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    474\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mpickle\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPickleError\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/cloudpickle/cloudpickle_fast.py\u001B[0m in \u001B[0;36mdumps\u001B[0;34m(obj, protocol, buffer_callback)\u001B[0m\n\u001B[1;32m     72\u001B[0m             )\n\u001B[0;32m---> 73\u001B[0;31m             \u001B[0mcp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdump\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobj\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     74\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mfile\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgetvalue\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/cloudpickle/cloudpickle_fast.py\u001B[0m in \u001B[0;36mdump\u001B[0;34m(self, obj)\u001B[0m\n\u001B[1;32m    562\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 563\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mPickler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdump\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mobj\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    564\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mRuntimeError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/context.py\u001B[0m in \u001B[0;36m__getnewargs__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    368\u001B[0m         \u001B[0;31m# This method is called when attempting to pickle SparkContext, which is always an error:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 369\u001B[0;31m         raise RuntimeError(\n\u001B[0m\u001B[1;32m    370\u001B[0m             \u001B[0;34m\"It appears that you are attempting to reference SparkContext from a broadcast \"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mRuntimeError\u001B[0m: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.\n\nDuring handling of the above exception, another exception occurred:\n\n\u001B[0;31mPicklingError\u001B[0m                             Traceback (most recent call last)\n\u001B[0;32m<command-2161344284055036>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mpredictionAndLabels\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mvalData\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mlp\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mfloat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfeatures\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlabel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mmetrics2\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mMulticlassMetrics\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpredictionAndLabels\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0mprec\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmetrics2\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprecision\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"precision: %.2f%%\"\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0mprec\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0;36m100\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/mllib/evaluation.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, predictionAndLabels)\u001B[0m\n\u001B[1;32m    268\u001B[0m         \u001B[0msc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpredictionAndLabels\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mctx\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    269\u001B[0m         \u001B[0msql_ctx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mSQLContext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgetOrCreate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 270\u001B[0;31m         \u001B[0mnumCol\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpredictionAndLabels\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfirst\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    271\u001B[0m         schema = StructType([\n\u001B[1;32m    272\u001B[0m             \u001B[0mStructField\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"prediction\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mDoubleType\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnullable\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/rdd.py\u001B[0m in \u001B[0;36mfirst\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1618\u001B[0m         \u001B[0mValueError\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mRDD\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0mempty\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1619\u001B[0m         \"\"\"\n\u001B[0;32m-> 1620\u001B[0;31m         \u001B[0mrs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtake\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1621\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mrs\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1622\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mrs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/rdd.py\u001B[0m in \u001B[0;36mtake\u001B[0;34m(self, num)\u001B[0m\n\u001B[1;32m   1598\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1599\u001B[0m             \u001B[0mp\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpartsScanned\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpartsScanned\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mnumPartsToTry\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtotalParts\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1600\u001B[0;31m             \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcontext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrunJob\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtakeUpToNumLeft\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mp\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1601\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1602\u001B[0m             \u001B[0mitems\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mres\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/context.py\u001B[0m in \u001B[0;36mrunJob\u001B[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001B[0m\n\u001B[1;32m   1333\u001B[0m             \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1334\u001B[0m                 \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mremove\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilename\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1335\u001B[0;31m         \u001B[0msock_info\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPythonRDD\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrunJob\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jsc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmappedRDD\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jrdd\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpartitions\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1336\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_load_from_socket\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msock_info\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmappedRDD\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jrdd_deserializer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1337\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/rdd.py\u001B[0m in \u001B[0;36m_jrdd\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   2983\u001B[0m             \u001B[0mprofiler\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2984\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2985\u001B[0;31m         wrapped_func = _wrap_function(self.ctx, self.func, self._prev_jrdd_deserializer,\n\u001B[0m\u001B[1;32m   2986\u001B[0m                                       self._jrdd_deserializer, profiler)\n\u001B[1;32m   2987\u001B[0m         python_rdd = self.ctx._jvm.PythonRDD(self._prev_jrdd.rdd(), wrapped_func,\n\n\u001B[0;32m/databricks/spark/python/pyspark/rdd.py\u001B[0m in \u001B[0;36m_wrap_function\u001B[0;34m(sc, func, deserializer, serializer, profiler)\u001B[0m\n\u001B[1;32m   2862\u001B[0m     \u001B[0;32massert\u001B[0m \u001B[0mserializer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"serializer should not be empty\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2863\u001B[0m     \u001B[0mcommand\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mfunc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mprofiler\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdeserializer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mserializer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2864\u001B[0;31m     \u001B[0mpickled_command\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbroadcast_vars\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0menv\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mincludes\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_prepare_for_python_RDD\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2865\u001B[0m     return sc._jvm.PythonFunction(bytearray(pickled_command), env, includes, sc.pythonExec,\n\u001B[1;32m   2866\u001B[0m                                   sc.pythonVer, broadcast_vars, sc._javaAccumulator)\n\n\u001B[0;32m/databricks/spark/python/pyspark/rdd.py\u001B[0m in \u001B[0;36m_prepare_for_python_RDD\u001B[0;34m(sc, command)\u001B[0m\n\u001B[1;32m   2848\u001B[0m     \u001B[0;31m# the serialized command will be compressed by broadcast\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2849\u001B[0m     \u001B[0mser\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mCloudPickleSerializer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2850\u001B[0;31m     \u001B[0mpickled_command\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mser\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdumps\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2851\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpickled_command\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0msc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPythonUtils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgetBroadcastThreshold\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jsc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m  \u001B[0;31m# Default 1M\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2852\u001B[0m         \u001B[0;31m# The broadcast will have same life cycle as created PythonRDD\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/serializers.py\u001B[0m in \u001B[0;36mdumps\u001B[0;34m(self, obj)\u001B[0m\n\u001B[1;32m    481\u001B[0m                 \u001B[0mmsg\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"Could not serialize object: %s: %s\"\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__class__\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__name__\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0memsg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    482\u001B[0m             \u001B[0mprint_exec\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msys\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstderr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 483\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0mpickle\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPicklingError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmsg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    484\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    485\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mPicklingError\u001B[0m: Could not serialize object: RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.","errorSummary":"<span class='ansi-red-fg'>PicklingError</span>: Could not serialize object: RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)\n\u001B[0;32m/databricks/spark/python/pyspark/serializers.py\u001B[0m in \u001B[0;36mdumps\u001B[0;34m(self, obj)\u001B[0m\n\u001B[1;32m    472\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 473\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mcloudpickle\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdumps\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobj\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpickle_protocol\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    474\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mpickle\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPickleError\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/cloudpickle/cloudpickle_fast.py\u001B[0m in \u001B[0;36mdumps\u001B[0;34m(obj, protocol, buffer_callback)\u001B[0m\n\u001B[1;32m     72\u001B[0m             )\n\u001B[0;32m---> 73\u001B[0;31m             \u001B[0mcp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdump\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobj\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     74\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mfile\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgetvalue\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/cloudpickle/cloudpickle_fast.py\u001B[0m in \u001B[0;36mdump\u001B[0;34m(self, obj)\u001B[0m\n\u001B[1;32m    562\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 563\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mPickler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdump\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mobj\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    564\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mRuntimeError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/context.py\u001B[0m in \u001B[0;36m__getnewargs__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    368\u001B[0m         \u001B[0;31m# This method is called when attempting to pickle SparkContext, which is always an error:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 369\u001B[0;31m         raise RuntimeError(\n\u001B[0m\u001B[1;32m    370\u001B[0m             \u001B[0;34m\"It appears that you are attempting to reference SparkContext from a broadcast \"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mRuntimeError\u001B[0m: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.\n\nDuring handling of the above exception, another exception occurred:\n\n\u001B[0;31mPicklingError\u001B[0m                             Traceback (most recent call last)\n\u001B[0;32m<command-2161344284055036>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mpredictionAndLabels\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mvalData\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mlp\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mfloat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfeatures\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlabel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mmetrics2\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mMulticlassMetrics\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpredictionAndLabels\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0mprec\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmetrics2\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprecision\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"precision: %.2f%%\"\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0mprec\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0;36m100\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/mllib/evaluation.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, predictionAndLabels)\u001B[0m\n\u001B[1;32m    268\u001B[0m         \u001B[0msc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpredictionAndLabels\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mctx\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    269\u001B[0m         \u001B[0msql_ctx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mSQLContext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgetOrCreate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 270\u001B[0;31m         \u001B[0mnumCol\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpredictionAndLabels\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfirst\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    271\u001B[0m         schema = StructType([\n\u001B[1;32m    272\u001B[0m             \u001B[0mStructField\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"prediction\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mDoubleType\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnullable\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/rdd.py\u001B[0m in \u001B[0;36mfirst\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1618\u001B[0m         \u001B[0mValueError\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mRDD\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0mempty\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1619\u001B[0m         \"\"\"\n\u001B[0;32m-> 1620\u001B[0;31m         \u001B[0mrs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtake\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1621\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mrs\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1622\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mrs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/rdd.py\u001B[0m in \u001B[0;36mtake\u001B[0;34m(self, num)\u001B[0m\n\u001B[1;32m   1598\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1599\u001B[0m             \u001B[0mp\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpartsScanned\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpartsScanned\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mnumPartsToTry\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtotalParts\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1600\u001B[0;31m             \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcontext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrunJob\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtakeUpToNumLeft\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mp\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1601\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1602\u001B[0m             \u001B[0mitems\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mres\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/context.py\u001B[0m in \u001B[0;36mrunJob\u001B[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001B[0m\n\u001B[1;32m   1333\u001B[0m             \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1334\u001B[0m                 \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mremove\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilename\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1335\u001B[0;31m         \u001B[0msock_info\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPythonRDD\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrunJob\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jsc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmappedRDD\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jrdd\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpartitions\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1336\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_load_from_socket\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msock_info\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmappedRDD\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jrdd_deserializer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1337\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/rdd.py\u001B[0m in \u001B[0;36m_jrdd\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   2983\u001B[0m             \u001B[0mprofiler\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2984\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2985\u001B[0;31m         wrapped_func = _wrap_function(self.ctx, self.func, self._prev_jrdd_deserializer,\n\u001B[0m\u001B[1;32m   2986\u001B[0m                                       self._jrdd_deserializer, profiler)\n\u001B[1;32m   2987\u001B[0m         python_rdd = self.ctx._jvm.PythonRDD(self._prev_jrdd.rdd(), wrapped_func,\n\n\u001B[0;32m/databricks/spark/python/pyspark/rdd.py\u001B[0m in \u001B[0;36m_wrap_function\u001B[0;34m(sc, func, deserializer, serializer, profiler)\u001B[0m\n\u001B[1;32m   2862\u001B[0m     \u001B[0;32massert\u001B[0m \u001B[0mserializer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"serializer should not be empty\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2863\u001B[0m     \u001B[0mcommand\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mfunc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mprofiler\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdeserializer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mserializer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2864\u001B[0;31m     \u001B[0mpickled_command\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbroadcast_vars\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0menv\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mincludes\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_prepare_for_python_RDD\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2865\u001B[0m     return sc._jvm.PythonFunction(bytearray(pickled_command), env, includes, sc.pythonExec,\n\u001B[1;32m   2866\u001B[0m                                   sc.pythonVer, broadcast_vars, sc._javaAccumulator)\n\n\u001B[0;32m/databricks/spark/python/pyspark/rdd.py\u001B[0m in \u001B[0;36m_prepare_for_python_RDD\u001B[0;34m(sc, command)\u001B[0m\n\u001B[1;32m   2848\u001B[0m     \u001B[0;31m# the serialized command will be compressed by broadcast\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2849\u001B[0m     \u001B[0mser\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mCloudPickleSerializer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2850\u001B[0;31m     \u001B[0mpickled_command\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mser\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdumps\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2851\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpickled_command\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0msc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPythonUtils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgetBroadcastThreshold\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jsc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m  \u001B[0;31m# Default 1M\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2852\u001B[0m         \u001B[0;31m# The broadcast will have same life cycle as created PythonRDD\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/serializers.py\u001B[0m in \u001B[0;36mdumps\u001B[0;34m(self, obj)\u001B[0m\n\u001B[1;32m    481\u001B[0m                 \u001B[0mmsg\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"Could not serialize object: %s: %s\"\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__class__\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__name__\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0memsg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    482\u001B[0m             \u001B[0mprint_exec\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msys\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstderr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 483\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0mpickle\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPicklingError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmsg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    484\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    485\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mPicklingError\u001B[0m: Could not serialize object: RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063."]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Proj1","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":4141706444095394}},"nbformat":4,"nbformat_minor":0}
